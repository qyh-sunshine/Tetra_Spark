{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "peripheral-disease",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "import gc\n",
    "#findspark.init() \n",
    "SPARK_HOME='/opt/cloudera/parcels/CDH/lib/spark'\n",
    "findspark.init(SPARK_HOME)\n",
    "\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import codecs\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import create_map,concat_ws,size,array_union,flatten,array_sort,coalesce,broadcast,collect_list, collect_set, udf, array_remove, log, lit, first, col, array, sort_array,split, explode, desc, asc, row_number,isnan, when, count\n",
    "from pyspark.sql.types import *\n",
    "import rtree\n",
    "from pyspark.sql import Window\n",
    "import geofeather\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType, ArrayType, MapType\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.core.SpatialRDD import SpatialRDD, PointRDD, CircleRDD, PolygonRDD, LineStringRDD\n",
    "from sedona.core.enums import FileDataSplitter\n",
    "from sedona.utils.adapter import Adapter\n",
    "from sedona.core.spatialOperator import KNNQuery\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "from sedona.core.spatialOperator import JoinQueryRaw\n",
    "from sedona.core.spatialOperator import RangeQuery\n",
    "from sedona.core.spatialOperator import RangeQueryRaw\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.core.formatMapper import WkbReader\n",
    "from sedona.core.formatMapper import WktReader\n",
    "from sedona.core.formatMapper import GeoJsonReader\n",
    "from sedona.sql.types import GeometryType\n",
    "from sedona.core.enums import GridType\n",
    "from sedona.core.SpatialRDD import RectangleRDD\n",
    "from sedona.core.enums import IndexType\n",
    "from sedona.core.geom.envelope import Envelope\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = \"./environment/bin/python\"\n",
    "os.environ['YARN_CONF_DIR'] = \"/opt/cloudera/parcels/CDH/lib/spark/conf/yarn-conf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "tin_file = input(\"Here is a programe to extract boundary relations, please input the absolute or relative path to your .ts file:\")\n",
    "\n",
    "# get the directory, basename of the input file\n",
    "print(\"\\n********************\")\n",
    "tin_directory = os.path.dirname(tin_file)\n",
    "print(\"tin_directory: \", tin_directory)\n",
    "\n",
    "directory_type = input(\"Is the data stored in hdfs(0) or Tri_data(1) or Tetra_data (2):\") or \"2\"\n",
    "\n",
    "if directory_type == '0':\n",
    "    directory = 'hdfs_data'\n",
    "elif directory_type == '1':\n",
    "    directory = 'Tri_data'\n",
    "else:\n",
    "    directory = 'Tetra_data'\n",
    "    \n",
    "tin_basename = os.path.basename(tin_file) # input_vertices_2.off\n",
    "print(\"tin_basename: \", tin_basename)\n",
    "\n",
    "tin_filename = os.path.splitext(tin_basename)[0] # input_vertices_2\n",
    "print(\"tin_filename: \", tin_filename)\n",
    "\n",
    "tin_extension = os.path.splitext(tin_basename)[1] # .off\n",
    "print(\"tin_extension: \", tin_extension)\n",
    "\n",
    "print(\"\\n********************\")\n",
    "print(\"This is a TIN file in \\\"%s\\\" format\" % tin_extension)\n",
    "\n",
    "filtra = 'yes'\n",
    "\n",
    "# allocate the number of executors, the number of cores per executor, and the amount of memory per executor\n",
    "Num_executor = input(\"spark.executor.instances:\") or \"64\"\n",
    "Num_core_per_executor = input(\"spark.executor.cores:\") or \"5\"\n",
    "Memory_executor = input(\"spark.executor.memory? Please end with 'g':\") or \"64g\"\n",
    "MemoryOverhead_executor = input(\"spark.executor.memoryOverhead? Please end with 'g':\") or \"8g\"\n",
    "\n",
    "Num_core_per_driver = input(\"spark.driver.cores:\") or \"5\"\n",
    "Memory_driver = input(\"spark.driver.memory? Please end with 'g':\") or \"64g\"\n",
    "MemoryOverhead_driver = input(\"spark.driver.memoryOverhead? Please end with 'g':\") or \"8g\"\n",
    "\n",
    "Num_shuffle_partitions = input(\"spark.sql.shuffle.partitions:\") or \"400\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "round-howard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_app_name: Tetra_Spark_A1_BasedOn_D3_compts_pureSpark_brain_11252025_1413\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "spark.executor.cores: # Number of concurrent tasks an executor can run, euqals to the number of cores to use on each executor\n",
    "spark.executor.instances: # Number of executors for the spark application\n",
    "spark.executor.memory: # Amount of memory to use for each executor that runs the task\n",
    "spark.executor.memoryOverhead:\n",
    "spark.driver.cores: # Number of cores to use for the driver process; the default number is 1\n",
    "spark.driver.memory: # Amount of memory to use for the driver\n",
    "spark.driver.maxResultSize: to define the maximum limit of the total size of the serialized result that a driver can store for each Spark collect action\n",
    "spark.default.parallelism: # Default number of partitions in RDDs returned by transformations like join, reduceByKey, and parallelize when not set by user. It can be set as spark.executor.instances * spark.executor.cores * 2\n",
    "spark.sql.shuffle.partitions: determine how many partitions are used when data is shuffled between nodes, e.g., joins or aggregations. usually 1~5 times of executor.instances * executor.cores\n",
    "spark.memory.storageFraction: determines the fraction of the heap space that is allocated to caching RDDs and DataFrames in memory.\n",
    "spark.kryoserializer.buffer.max: determine the maximum of data that can be serialized at once; this must be larger than any object we attempt to serialize\n",
    "spark.rpc.message.maxSize: # Maximum message size (in MiB) to allow in \"control plane\" communication; generally only applies to map output size information sent between executors and the driver. To communicate between the nodes, Spark uses a protocol called RPC (Remote Procedure Call), which sends messages back and forth. The spark.rpc.message.maxSize parameter limits how big these messages can be. \n",
    "spark.sql.broadcastTimeout: Spark will wait for this amount of time before giving up on broadcasting a table. Broadcasting can take a long time if the table is large or if there is a shuffle operation before it.\n",
    "spark.sql.autoBroadcastJoinThreshold: Spark will broadcast a table to all worker nodes when performing a join if its size is less than this value; -1 means disabling broadcasting\n",
    "'''\n",
    "\n",
    "date = time.strftime(\"%m,%d,%Y\")\n",
    "date_name = date.split(',')[0] + date.split(',')[1] + date.split(',')[2]\n",
    "\n",
    "hour = time.strftime(\"%H,%M\")\n",
    "hour_name = hour.split(',')[0] + hour.split(',')[1]\n",
    "\n",
    "spark_app_name = \"Tetra_Spark_A1_BasedOn_D3_compts_pureSpark_\" + tin_filename + '_' + date_name + '_' + hour_name\n",
    "print(\"spark_app_name:\", spark_app_name)\n",
    "\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(spark_app_name) \\\n",
    ".master('yarn') \\\n",
    ".config(\"spark.serializer\", KryoSerializer.getName) \\\n",
    ".config('spark.jars','sedona-core-2.4_2.11-1.0.0-incubating.jar,sedona-sql-2.4_2.11-1.0.0-incubating.jar,sedona-python-adapter-2.4_2.11-1.0.0-incubating.jar,sedona-viz-2.4_2.11-1.0.0-incubating.jar,geotools-wrapper-geotools-24.0.jar,graphframes-0.8.0-spark2.4-s_2.11.jar') \\\n",
    ".config('spark.executor.cores', Num_core_per_executor) \\\n",
    ".config('spark.executor.instances', Num_executor) \\\n",
    ".config('spark.executor.memory', Memory_executor) \\\n",
    ".config('spark.executor.memoryOverhead', MemoryOverhead_executor) \\\n",
    ".config('spark.driver.cores', Num_core_per_driver) \\\n",
    ".config('spark.driver.memory', Memory_driver) \\\n",
    ".config('spark.driver.memoryOverhead', MemoryOverhead_driver) \\\n",
    ".config('spark.driver.maxResultSize', '0') \\\n",
    ".config('spark.dynamicAllocation.enabled', 'false') \\\n",
    ".config('spark.network.timeout', '10000001s') \\\n",
    ".config('spark.executor.heartbeatInterval', '10000000s') \\\n",
    ".config('spark.sql.shuffle.partitions', Num_shuffle_partitions) \\\n",
    ".config(\"spark.default.parallelism\", '128') \\\n",
    ".config(\"spark.kryoserializer.buffer.max\", \"1024mb\") \\\n",
    ".config('spark.rpc.message.maxSize', '256') \\\n",
    ".config(\"spark.sql.broadcastTimeout\", \"36000\") \\\n",
    ".config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    ".config(\"spark.sql.objectHashAggregate.sortBased.fallbackThreshold\", \"-1\") \\\n",
    ".config('spark.yarn.dist.archives', '/path/to/py37.tar.gz#environment') \\\n",
    ".config(\"spark.python.profile\", \"true\") \\\n",
    ".config(\"spark.eventLog.enabled\", \"true\") \\\n",
    ".config(\"spark.eventLog.logStageExecutorMetrics\", \"true\") \\\n",
    ".config(\"spark.blacklist.enabled\", \"false\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brave-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphframes\n",
    "from graphframes import GraphFrame\n",
    "from graphframes import *\n",
    "from graphframes.lib import Pregel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-tours",
   "metadata": {},
   "source": [
    "### read input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "similar-electron",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tetra_order: integer (nullable = true)\n",
      " |-- r1: integer (nullable = true)\n",
      " |-- r2: integer (nullable = true)\n",
      " |-- r3: integer (nullable = true)\n",
      " |-- r4: integer (nullable = true)\n",
      " |-- r1_ele: float (nullable = true)\n",
      " |-- r2_ele: float (nullable = true)\n",
      " |-- r3_ele: float (nullable = true)\n",
      " |-- r4_ele: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read_tetra_order() is a function used to read tetrahedron from a csv file\n",
    "def read_tetra_order(hdfs_tetra_origin):\n",
    "    '''\n",
    "    this function has two input parameters.\n",
    "    filtra: 'yes' or 'no', yes means that the input csv file is ordered by default\n",
    "    directory: a string denoting the directory to a tetrahedra file\n",
    "    tin_filename: a string denoting the file name of a tetrahedra extension, e.g., 827_monviso\n",
    "    '''\n",
    "        \n",
    "    schema_tetra_origin = StructType([ \\\n",
    "        StructField(\"tetra_order\",IntegerType(),True), \\\n",
    "        StructField(\"r1\",IntegerType(),True), \\\n",
    "        StructField(\"r2\",IntegerType(),True), \\\n",
    "        StructField(\"r3\",IntegerType(),True), \\\n",
    "        StructField(\"r4\",IntegerType(),True), \\\n",
    "        StructField(\"r1_ele\",FloatType(),True), \\\n",
    "        StructField(\"r2_ele\",FloatType(),True), \\\n",
    "        StructField(\"r3_ele\",FloatType(),True), \\\n",
    "        StructField(\"r4_ele\",FloatType(),True) \\\n",
    "      ])\n",
    "\n",
    "    df_tetra_origin = spark.read.format(\"csv\") \\\n",
    "          .option(\"header\", False) \\\n",
    "          .schema(schema_tetra_origin)\\\n",
    "          .load(hdfs_tetra_origin)\n",
    "        \n",
    "    return df_tetra_origin\n",
    "\n",
    "\n",
    "# read tetrahedra\n",
    "hdfs_tetra_origin = directory + \"/\" + tin_filename + '_filtra_tetra_sort.csv'\n",
    "\n",
    "df_tetra_order = read_tetra_order(hdfs_tetra_origin)\n",
    "df_tetra_order.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-fault",
   "metadata": {},
   "source": [
    "### globally obtain VT relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mobile-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get VT directly from DF_T\n",
    "def get_VT(df_tetra_order):\n",
    "    df_tetra_order = df_tetra_order.withColumn(\"tetra\", sort_array(F.array(\"r1\", \"r2\", \"r3\", \"r4\"), False))\n",
    "    df_VT_init_1 = df_tetra_order.select(\"r1\",\"tetra\")\n",
    "    df_VT_init_2 = df_tetra_order.select(\"r2\",\"tetra\")\n",
    "    df_VT_init_3 = df_tetra_order.select(\"r3\",\"tetra\")\n",
    "    df_VT_init_4 = df_tetra_order.select(\"r4\",\"tetra\")\n",
    "    \n",
    "    df_VT_union12 = df_VT_init_1.union(df_VT_init_2)\n",
    "    df_VT_union123 = df_VT_union12.union(df_VT_init_3)\n",
    "    df_VT_union1234 = df_VT_union123.union(df_VT_init_4)\n",
    "    \n",
    "    df_VT = df_VT_union1234.groupBy('r1').agg(collect_list('tetra').alias('VT'))\n",
    "    df_VT = df_VT.withColumnRenamed('r1', 'Ver')\n",
    "    \n",
    "    return df_VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "specific-patio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_VT = get_VT(df_tetra_order)\n",
    "\n",
    "df_VT.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-broadway",
   "metadata": {},
   "source": [
    "### globally obtain VF relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "professional-jones",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- r1: integer (nullable = true)\n",
      " |-- multi_f1: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- multi_f2: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- multi_f3: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function to get VF from DF_T\n",
    "def get_VF_init(df_tetra_order):\n",
    "    df_VF_1 = df_tetra_order.withColumn(\"f1\", sort_array(F.array(\"r1\", \"r2\", \"r3\"), False)).withColumn(\"f2\", sort_array(F.array(\"r1\", \"r2\", \"r4\"), False)).withColumn(\"f3\", sort_array(F.array(\"r1\", \"r3\", \"r4\"), False)).drop('r2', 'r3', 'r4')\n",
    "    df_VF_2 = df_tetra_order.withColumn(\"f1\", sort_array(F.array(\"r2\", \"r1\", \"r3\"), False)).withColumn(\"f2\", sort_array(F.array(\"r2\", \"r1\", \"r4\"), False)).withColumn(\"f3\", sort_array(F.array(\"r2\", \"r3\", \"r4\"), False)).drop('r1', 'r3', 'r4')\n",
    "    df_VF_3 = df_tetra_order.withColumn(\"f1\", sort_array(F.array(\"r3\", \"r1\", \"r2\"), False)).withColumn(\"f2\", sort_array(F.array(\"r3\", \"r1\", \"r4\"), False)).withColumn(\"f3\", sort_array(F.array(\"r3\", \"r2\", \"r4\"), False)).drop('r1', 'r2', 'r4')\n",
    "    df_VF_4 = df_tetra_order.withColumn(\"f1\", sort_array(F.array(\"r4\", \"r1\", \"r2\"), False)).withColumn(\"f2\", sort_array(F.array(\"r4\", \"r1\", \"r3\"), False)).withColumn(\"f3\", sort_array(F.array(\"r4\", \"r2\", \"r3\"), False)).drop('r1', 'r2', 'r3')\n",
    "    \n",
    "    df_VF_union12 = df_VF_1.union(df_VF_2)\n",
    "    df_VF_union123 = df_VF_union12.union(df_VF_3)\n",
    "    df_VF_union1234 = df_VF_union123.union(df_VF_4)\n",
    "    \n",
    "    df_VF_init = df_VF_union1234.groupBy('r1').agg(collect_set('f1').alias('multi_f1'), collect_set('f2').alias('multi_f2'), collect_set('f3').alias('multi_f3'))\n",
    "    return df_VF_init\n",
    "\n",
    "df_VF_init = get_VF_init(df_tetra_order)\n",
    "\n",
    "# df_VE_init.cache()\n",
    "\n",
    "df_VF_init.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "proprietary-observer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VF: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obtain VF relation\n",
    "def get_VF(multi_f1, multi_f2, multi_f3):\n",
    "# get_VF is used to obtain a complete VF relation from the partial VF relations\n",
    "# multi_f1: partial VF relation\n",
    "# multi_f2: partial VF relation\n",
    "# multi_f3: partial VF relation\n",
    "\n",
    "    faces = set()\n",
    "    for f in multi_f1:\n",
    "        faces.add(tuple(f))\n",
    "        \n",
    "    for f in multi_f2:\n",
    "        faces.add(tuple(f))\n",
    "        \n",
    "    for f in multi_f3:\n",
    "        faces.add(tuple(f))\n",
    "    \n",
    "    faces_list = sorted(faces) # save more time when using list(faces)\n",
    "    \n",
    "    return faces_list\n",
    "\n",
    "# convert a function to an udf and determine the return type\n",
    "# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "get_VF_udf = udf(get_VF, ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "df_VF = df_VF_init.withColumn(\"VF\", get_VF_udf(df_VF_init.multi_f1, df_VF_init.multi_f2, df_VF_init.multi_f3)).drop('multi_f1', 'multi_f2', 'multi_f3')\n",
    "df_VF = df_VF.withColumnRenamed('r1', 'Ver')\n",
    "\n",
    "df_VF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-melbourne",
   "metadata": {},
   "source": [
    "### globally obtain VE relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "increased-disorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- r1: integer (nullable = true)\n",
      " |-- multi_e1: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- multi_e2: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- multi_e3: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function to get VE from DF_T\n",
    "def get_VE_init(df_tetra_order):\n",
    "    df_VE_1 = df_tetra_order.withColumn(\"e1\", sort_array(F.array(\"r1\", \"r2\"), False)).withColumn(\"e2\", sort_array(F.array(\"r1\", \"r3\"), False)).withColumn(\"e3\", sort_array(F.array(\"r1\", \"r4\"), False)).drop('r2', 'r3', 'r4')\n",
    "    df_VE_2 = df_tetra_order.withColumn(\"e1\", sort_array(F.array(\"r2\", \"r1\"), False)).withColumn(\"e2\", sort_array(F.array(\"r2\", \"r3\"), False)).withColumn(\"e3\", sort_array(F.array(\"r2\", \"r4\"), False)).drop('r1', 'r3', 'r4')\n",
    "    df_VE_3 = df_tetra_order.withColumn(\"e1\", sort_array(F.array(\"r3\", \"r1\"), False)).withColumn(\"e2\", sort_array(F.array(\"r3\", \"r2\"), False)).withColumn(\"e3\", sort_array(F.array(\"r3\", \"r4\"), False)).drop('r1', 'r2', 'r4')\n",
    "    df_VE_4 = df_tetra_order.withColumn(\"e1\", sort_array(F.array(\"r4\", \"r1\"), False)).withColumn(\"e2\", sort_array(F.array(\"r4\", \"r2\"), False)).withColumn(\"e3\", sort_array(F.array(\"r4\", \"r3\"), False)).drop('r1', 'r2', 'r3')\n",
    "    \n",
    "    df_VE_union1234 = df_VE_1.union(df_VE_2).union(df_VE_3).union(df_VE_4)\n",
    "    \n",
    "    df_VE_init = df_VE_union1234.groupBy('r1').agg(collect_set('e1').alias('multi_e1'), collect_set('e2').alias('multi_e2'), collect_set('e3').alias('multi_e3'))\n",
    "    return df_VE_init\n",
    "\n",
    "df_VE_init = get_VE_init(df_tetra_order)\n",
    "\n",
    "# df_VE_init.cache()\n",
    "\n",
    "df_VE_init.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "intellectual-trinity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VE: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obtain VE relation\n",
    "def get_VE(multi_e1, multi_e2, multi_e3):\n",
    "# get_VE is used to obtain the partial VE relation\n",
    "# multi_e1: partial VE relation\n",
    "# multi_e2: partial VE relation\n",
    "    edges = set()\n",
    "    for e in multi_e1:\n",
    "        edges.add(tuple(e))\n",
    "        \n",
    "    for e in multi_e2:\n",
    "        edges.add(tuple(e))\n",
    "        \n",
    "    for e in multi_e3:\n",
    "        edges.add(tuple(e))\n",
    "    \n",
    "    edges_list = sorted(edges) # sorted(pt_set, reverse=False), False in ascending order while True in descending order\n",
    "    \n",
    "    return edges_list\n",
    "\n",
    "# convert a function to an udf and determine the return type\n",
    "# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "get_VE_udf = udf(get_VE, ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "df_VE = df_VE_init.withColumn(\"VE\", get_VE_udf(df_VE_init.multi_e1, df_VE_init.multi_e2, df_VE_init.multi_e3)).drop('multi_e1', 'multi_e2', 'multi_e3')\n",
    "df_VE = df_VE.withColumnRenamed('r1', 'Ver')\n",
    "\n",
    "df_VE.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-metro",
   "metadata": {},
   "source": [
    "### globally obtain EF relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "growing-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get TE relation\n",
    "def get_TE_from_T(df_tetra_order):\n",
    "    df_TE = df_tetra_order.withColumn(\"tetra\", sort_array(F.array(\"r1\", \"r2\", \"r3\", \"r4\"), False)).withColumn(\"e1\", sort_array(F.array(\"r1\", \"r2\",), False)).withColumn(\"e2\", sort_array(F.array(\"r1\", \"r3\",), False)).withColumn(\"e3\", sort_array(F.array(\"r1\", \"r4\",), False)).withColumn(\"e4\", sort_array(F.array(\"r2\", \"r3\",), False)).withColumn(\"e5\", sort_array(F.array(\"r2\", \"r4\",), False)).withColumn(\"e6\", sort_array(F.array(\"r3\", \"r4\",), False))\n",
    "    df_TE = df_TE.select('tetra', 'e1', 'e2', 'e3', 'e4', 'e5', 'e6')\n",
    "    \n",
    "    return df_TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "binding-curve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- edge: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get edges from DF_T\n",
    "\n",
    "df_TE = get_TE_from_T(df_tetra_order)\n",
    "\n",
    "df_E1 = df_TE.select('e1')\n",
    "df_E2 = df_TE.select('e2')\n",
    "df_E3 = df_TE.select('e3')\n",
    "df_E4 = df_TE.select('e4')\n",
    "df_E5 = df_TE.select('e5')\n",
    "df_E6 = df_TE.select('e6')\n",
    "\n",
    "df_Edges = df_E1.union(df_E2).union(df_E3).union(df_E4).union(df_E5).union(df_E6)\n",
    "df_Edges = df_Edges.withColumnRenamed('e1', 'edge')\n",
    "df_Edges = df_Edges.distinct()\n",
    "df_Edges.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "peripheral-brighton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- edge: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- r1: integer (nullable = true)\n",
      " |-- r2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_EV = df_Edges.withColumn('r1', df_Edges.edge[0]).withColumn('r2', df_Edges.edge[1])\n",
    "df_EV.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-facing",
   "metadata": {},
   "source": [
    "##### join EV and VF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "saved-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EF_init_1 = df_EV.join(df_VF, df_EV.r1==df_VF.Ver).select(\"edge\", col(\"VF\").alias(\"VF_1\"), \"r2\")\n",
    "df_EF_init_2 = df_EF_init_1.join(df_VF, df_EF_init_1.r2==df_VF.Ver).select(\"edge\", \"VF_1\", col(\"VF\").alias(\"VF_2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "accurate-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_EF(edge, VF_1, VF_2):\n",
    "    EF = set()\n",
    "    edge_set = set(edge)\n",
    "    for face in VF_1:\n",
    "        face_set = set(face)\n",
    "        if edge_set.issubset(face_set):\n",
    "            ET.add(tuple(face))\n",
    "            \n",
    "    for face in VF_2:\n",
    "        face_set = set(face)\n",
    "        if edge_set.issubset(face_set):\n",
    "            ET.add(tuple(face))\n",
    "            \n",
    "    EF_list = list(EF)\n",
    "    \n",
    "    return EF_list\n",
    "\n",
    "get_EF_udf = udf(get_EF, ArrayType(ArrayType(IntegerType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "endangered-stupid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Edge: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- EF: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_EF = df_EF_init_2.withColumn(\"EF\", get_EF_udf(df_EF_init_2.edge, df_EF_init_2.VF_1, df_EF_init_2.VF_2)).select(col(\"edge\").alias(\"Edge\"), \"EF\")\n",
    "df_EF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-chicken",
   "metadata": {},
   "source": [
    "##### get EF where edges incident to the same vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "purple-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- Edge: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- EF: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_VE_expl = df_VE.select(\"Ver\", explode(\"VE\").alias(\"single_edge\"))\n",
    "\n",
    "df_EF_expected_init = df_VE_expl.join(df_EF, df_VE_expl.single_edge == df_EF.Edge).drop('single_edge')\n",
    "df_EF_expected_init.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "modern-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_EF(Edge, EF):\n",
    "    EF_ary = [[Edge], EF]\n",
    "    \n",
    "    return EF_ary\n",
    "\n",
    "union_EF_udf = udf(union_EF, ArrayType(ArrayType(ArrayType(IntegerType()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "intended-hughes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- EF_final: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_EF_expected_udf = df_EF_expected_init.withColumn(\"EF_expected\", union_EF_udf(df_EF_expected_init.Edge, df_EF_expected_init.EF))\n",
    "df_EF_expected_udf = df_EF_expected_udf.select(\"Ver\", \"EF_expected\")\n",
    "\n",
    "df_EF_expected = df_EF_expected_udf.groupby(\"Ver\").agg(collect_list('EF_expected').alias('EF_final'))\n",
    "df_EF_expected.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-protocol",
   "metadata": {},
   "source": [
    "#### globally obtain FT relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-browser",
   "metadata": {},
   "source": [
    "#### first get FV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "democratic-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get TF relation\n",
    "def get_TF_from_T(df_tetra_order):\n",
    "    df_TF = df_tetra_order.withColumn(\"tetra\", sort_array(F.array(\"r1\", \"r2\", \"r3\", \"r4\"), False)).withColumn(\"f1\", sort_array(F.array(\"r1\", \"r2\", \"r3\"), False)).withColumn(\"f2\", sort_array(F.array(\"r1\", \"r2\", \"r4\"), False)).withColumn(\"f3\", sort_array(F.array(\"r1\", \"r3\", \"r4\"), False)).withColumn(\"f4\", sort_array(F.array(\"r2\", \"r3\", \"r4\"), False))\n",
    "    df_TF = df_TF.select('tetra', 'f1', 'f2', 'f3', 'f4')\n",
    "    \n",
    "    return df_TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "taken-storm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- face: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get faces from DF_T\n",
    "\n",
    "df_TF = get_TF_from_T(df_tetra_order)\n",
    "\n",
    "df_F1 = df_TF.select('f1')\n",
    "df_F2 = df_TF.select('f2')\n",
    "df_F3 = df_TF.select('f3')\n",
    "df_F4 = df_TF.select('f4')\n",
    "\n",
    "df_Faces = df_F1.union(df_F2).union(df_F3).union(df_F4)\n",
    "df_Faces = df_Faces.withColumnRenamed('f1', 'face')\n",
    "df_Faces = df_Faces.distinct()\n",
    "df_Faces.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "grateful-appeal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- face: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- r1: integer (nullable = true)\n",
      " |-- r2: integer (nullable = true)\n",
      " |-- r3: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_FV = df_Faces.withColumn('r1', df_Faces.face[0]).withColumn('r2', df_Faces.face[1]).withColumn('r3', df_Faces.face[2])\n",
    "df_FV.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-cartridge",
   "metadata": {},
   "source": [
    "##### then get VT, and join df_FV and df_VT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "creative-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FT_init_1 = df_FV.join(df_VT, df_FV.r1==df_VT.Ver).select(\"face\", col(\"VT\").alias(\"VT_1\"), \"r2\", \"r3\")\n",
    "df_FT_init_2 = df_FT_init_1.join(df_VT, df_FT_init_1.r2==df_VT.Ver).select(\"face\", \"VT_1\", col(\"VT\").alias(\"VT_2\"), \"r3\")\n",
    "df_FT_init_3 = df_FT_init_2.join(df_VT, df_FT_init_2.r3==df_VT.Ver).select(\"face\", \"VT_1\", \"VT_2\", col(\"VT\").alias(\"VT_3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aggressive-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_FT(face, VT_1, VT_2, VT_3):\n",
    "    FT = set()\n",
    "    face_set = set(face)\n",
    "    for tetra in VT_1:\n",
    "        tetra_set = set(tetra)\n",
    "        if face_set.issubset(tetra_set):\n",
    "            FT.add(tuple(tetra))\n",
    "            \n",
    "    for tetra in VT_2:\n",
    "        tetra_set = set(tetra)\n",
    "        if face_set.issubset(tetra_set):\n",
    "            FT.add(tuple(tetra))\n",
    "            \n",
    "    for tetra in VT_3:\n",
    "        tetra_set = set(tetra)\n",
    "        if face_set.issubset(tetra_set):\n",
    "            FT.add(tuple(tetra))\n",
    "            \n",
    "    FT_list = list(FT)\n",
    "    \n",
    "    return FT_list\n",
    "\n",
    "get_FT_udf = udf(get_FT, ArrayType(ArrayType(IntegerType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "korean-villa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Face: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- FT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_FT = df_FT_init_3.withColumn(\"FT\", get_FT_udf(df_FT_init_3.face, df_FT_init_3.VT_1, df_FT_init_3.VT_2, df_FT_init_3.VT_3)).select(col(\"face\").alias(\"Face\"), \"FT\")\n",
    "df_FT.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-examination",
   "metadata": {},
   "source": [
    "##### get FT where faces incident to the same vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "designing-salad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- Face: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- FT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_VF_expl = df_VF.select(\"Ver\", explode(\"VF\").alias(\"single_face\"))\n",
    "\n",
    "df_FT_expected_init = df_VF_expl.join(df_FT, df_VF_expl.single_face == df_FT.Face).drop('single_face')\n",
    "df_FT_expected_init.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "severe-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_FT(Face, FT):\n",
    "    FT_ary = [[Face], FT]\n",
    "    \n",
    "    return FT_ary\n",
    "\n",
    "union_FT_udf = udf(union_FT, ArrayType(ArrayType(ArrayType(IntegerType()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "intellectual-baseball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- FT_final: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_FT_expected_udf = df_FT_expected_init.withColumn(\"FT_expected\", union_FT_udf(df_FT_expected_init.Face, df_FT_expected_init.FT))\n",
    "df_FT_expected_udf = df_FT_expected_udf.select(\"Ver\", \"FT_expected\")\n",
    "\n",
    "df_FT_expected = df_FT_expected_udf.groupby(\"Ver\").agg(collect_list('FT_expected').alias('FT_final'))\n",
    "df_FT_expected.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-sacramento",
   "metadata": {},
   "source": [
    "### concatenate the relations in one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "structural-norman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- VF: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_VT_VF = df_VT.join(df_VF, df_VT.Ver == df_VF.Ver)\n",
    "df_VT_VF = df_VT_VF.select(df_VT.Ver, \"VT\", \"VF\")\n",
    "\n",
    "df_VT_VF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "roman-foster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- VF: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- VE: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_VT_VF_VE = df_VT_VF.join(df_VE, df_VT_VF.Ver == df_VE.Ver)\n",
    "df_VT_VF_VE = df_VT_VF_VE.select(df_VT_VF.Ver, \"VT\", \"VF\", \"VE\")\n",
    "\n",
    "df_VT_VF_VE.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-dance",
   "metadata": {},
   "source": [
    "##### concatenate the EF and FT relations in one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "referenced-yield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- VF: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- VE: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- EF_final: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_VT_VF_VE_EF = df_VT_VF_VE.join(df_EF_expected, df_VT_VF_VE.Ver == df_EF_expected.Ver)\n",
    "\n",
    "df_VT_VF_VE_EF = df_VT_VF_VE_EF.select(df_VT_VF_VE.Ver, \"VT\", \"VF\", \"VE\", \"EF_final\")\n",
    "df_VT_VF_VE_EF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "healthy-queue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- VF: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- VE: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- EF_final: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: integer (containsNull = true)\n",
      " |-- FT_final: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_VT_VF_VE_EF_FT = df_VT_VF_VE_EF.join(df_FT_expected, df_VT_VF_VE_EF.Ver == df_FT_expected.Ver)\n",
    "\n",
    "df_Forman_rela = df_VT_VF_VE_EF_FT.select(df_VT_VF_VE_EF.Ver, \"VT\", \"VF\", \"VE\", \"EF_final\", \"FT_final\")\n",
    "df_Forman_rela.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-episode",
   "metadata": {},
   "source": [
    "### compute the edges, faces, and tetrahedra within a lower star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "direct-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LS_EFT(Ver, VT, VF, VE):\n",
    "    LS_VT = []\n",
    "    LS_VF = []    \n",
    "    LS_VE = []   \n",
    "    for tetra in VT:\n",
    "        if Ver >= max(tetra):\n",
    "            LS_VT.append(tetra)\n",
    "    for face in VF:\n",
    "        if Ver >= max(face):\n",
    "            LS_VF.append(face)\n",
    "    for edge in VE:\n",
    "        if Ver >= max(edge):\n",
    "            LS_VE.append(edge)\n",
    "            \n",
    "    return LS_VT, LS_VF, LS_VE\n",
    "\n",
    "get_LS_EFT_schema = StructType([\n",
    "    StructField(\"LS_tetra\", ArrayType(ArrayType(IntegerType())),True), \n",
    "    StructField('LS_face',ArrayType(ArrayType(IntegerType())),True),\n",
    "    StructField('LS_edge',ArrayType(ArrayType(IntegerType())),True)\n",
    "])\n",
    "\n",
    "get_LS_EFT_udf = udf(get_LS_EFT, get_LS_EFT_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bigger-brazilian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- LS_VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- LS_VF: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- LS_VE: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_LS_EFT_init = df_Forman_rela.withColumn(\"LS_EFT\", get_LS_EFT_udf(df_Forman_rela.Ver, df_Forman_rela.VT, df_Forman_rela.VF, df_Forman_rela.VE))\n",
    "\n",
    "df_LS_EFT = df_LS_EFT_init.select('Ver', col(\"VT\"), col('LS_EFT.LS_tetra').alias('LS_VT'), col('LS_EFT.LS_face').alias('LS_VF'), col('LS_EFT.LS_edge').alias('LS_VE'))\n",
    "df_LS_EFT.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-skill",
   "metadata": {},
   "source": [
    "### compute Forman gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "plain-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "def get_Forman(Ver, LS_tetra, LS_face, LS_edge):\n",
    "    d = 3\n",
    "    k = 0\n",
    "    ST = deque() # ST stores k-simplices belonging to the lower star, the initial value is the vertex Ver\n",
    "    ST.append(Ver)\n",
    "    # LT = LS_face # LT stores the top simplices belonging to the lower star, here are faces\n",
    "    crit_cell = [] # crit_cell will store critical simplices\n",
    "    Forman_VE = [] # store VE pairs\n",
    "    Forman_EF = [] # store EF pairs\n",
    "    Forman_FT = [] # store FT pairs\n",
    "    \n",
    "    if (len(LS_edge)+len(LS_face)+len(LS_tetra)) == 0: # ver is a local minimum\n",
    "        crit_cell.append([Ver])\n",
    "        return crit_cell, Forman_VE, Forman_EF, Forman_FT\n",
    "    \n",
    "    def num_unpaired_edges(tri_inner, crit_cell_inner, Forman_vec_pair_VE_inner, Forman_vec_pair_EF_inner):\n",
    "        # compute the number of unpaired edges in tri_inner\n",
    "        edge0 = [tri_inner[0], tri_inner[1]]\n",
    "        edge1 = [tri_inner[0], tri_inner[2]]\n",
    "        \n",
    "        num_unpaired_edge = 2\n",
    "        paired_edge = [] # stores edges in crit_cell_inner, Forman_vec_pair_VE_inner, and Forman_vec_pair_EF_inner\n",
    "        for crit_inner_temp in crit_cell_inner:\n",
    "            if len(crit_inner_temp) == 2: # crit_inner_temp is a critical edge\n",
    "                paired_edge.append(crit_inner_temp)\n",
    "                \n",
    "        for pair_vec in Forman_vec_pair_VE_inner: # pair_vec is a vector, it can be (ver, edge) or (edge, face)\n",
    "            paired_edge.append(pair_vec[1])\n",
    "        for pair_vec in Forman_vec_pair_EF_inner: # pair_vec is a vector, it can be (ver, edge) or (edge, face)\n",
    "            paired_edge.append(pair_vec[0])\n",
    "                \n",
    "        potential_paired_edge = [edge0, edge1]\n",
    "        if edge0 in paired_edge:\n",
    "            num_unpaired_edge = num_unpaired_edge - 1\n",
    "            potential_paired_edge.remove(edge0)\n",
    "            \n",
    "        if edge1 in paired_edge:\n",
    "            num_unpaired_edge = num_unpaired_edge - 1\n",
    "            potential_paired_edge.remove(edge1)\n",
    "            \n",
    "        return num_unpaired_edge, potential_paired_edge\n",
    "    \n",
    "    def get_EF_pair(ST_inner, crit_cell_inner, Forman_VE_inner, Forman_EF_inner):\n",
    "        # this function is to get the EF pairs, E derives from CR while F derives from ST\n",
    "        for i in range(len(ST_inner)):\n",
    "            face = ST_inner[i]\n",
    "            num_unpaired_edge, potential_paired_edge = num_unpaired_edges(face, crit_cell_inner, Forman_VE_inner, Forman_EF_inner)\n",
    "            if num_unpaired_edge == 1:\n",
    "                return [potential_paired_edge[0], face] # return the EF pair\n",
    "            \n",
    "    def num_unpaired_faces(tetra_inner, crit_cell_inner, Forman_vec_pair_EF_inner, Forman_vec_pair_FT_inner):\n",
    "        # compute the number of unpaired faces in tetra_inner\n",
    "        face0 = [tetra_inner[0], tetra_inner[1], tetra_inner[2]]\n",
    "        face1 = [tetra_inner[0], tetra_inner[1], tetra_inner[3]]\n",
    "        face2 = [tetra_inner[0], tetra_inner[2], tetra_inner[3]]\n",
    "        \n",
    "        num_unpaired_face = 3\n",
    "        paired_face = [] # stores edges in crit_cell_inner, Forman_vec_pair_VE_inner, and Forman_vec_pair_EF_inner\n",
    "        for crit_inner_temp in crit_cell_inner:\n",
    "            if len(crit_inner_temp) == 3: # crit_inner_temp is a critical face\n",
    "                paired_face.append(crit_inner_temp)\n",
    "                \n",
    "        for pair_vec in Forman_vec_pair_EF_inner: # pair_vec is a vector, it can be (edge, face) or (face, tetra)\n",
    "            paired_face.append(pair_vec[1])\n",
    "        for pair_vec in Forman_vec_pair_FT_inner: # pair_vec is a vector, it can be (edge, face) or (face, tetra)\n",
    "            paired_face.append(pair_vec[0])\n",
    "                \n",
    "        potential_paired_face = [face0, face1, face2]\n",
    "        if face0 in paired_face:\n",
    "            num_unpaired_face = num_unpaired_face - 1\n",
    "            potential_paired_face.remove(face0)\n",
    "            \n",
    "        if face1 in paired_face:\n",
    "            num_unpaired_face = num_unpaired_face - 1\n",
    "            potential_paired_face.remove(face1)\n",
    "            \n",
    "        if face2 in paired_face:\n",
    "            num_unpaired_face = num_unpaired_face - 1\n",
    "            potential_paired_face.remove(face2)\n",
    "            \n",
    "        return num_unpaired_face, potential_paired_face       \n",
    "            \n",
    "    def get_FT_pair(ST_inner, crit_cell_inner, Forman_EF_inner, Forman_FT_inner):\n",
    "        # this function is to get the FT pairs, F derives from CR while T derives from ST\n",
    "        for i in range(len(ST_inner)):\n",
    "            tetra = ST_inner[i]\n",
    "            num_unpaired_face, potential_paired_face = num_unpaired_faces(tetra, crit_cell_inner, Forman_EF_inner, Forman_FT_inner)\n",
    "            if num_unpaired_face == 1:\n",
    "                return [potential_paired_face[0], tetra] # return the FT pair\n",
    "                \n",
    "        \n",
    "    \n",
    "    while k <= d: # d == 3 for a tetrahedral mesh\n",
    "        CR = ST # CR stores candidate critical simplices\n",
    "        k = k + 1        \n",
    "        if k == 1: # edges in the lower star\n",
    "            ST = deque()\n",
    "            for e in LS_edge:\n",
    "                ST.append(e) # assign LS_edge to ST\n",
    "        elif k == 2: # faces in the lower star\n",
    "            ST = deque()\n",
    "            for f in LS_face:\n",
    "                ST.append(f) # assign LS_face to ST\n",
    "        elif k == 3: # tetrahedra in the lower star\n",
    "            ST = deque()\n",
    "            for t in LS_tetra:\n",
    "                ST.append(t) # assign LS_tetra to ST\n",
    "        \n",
    "        while len(CR) > 0:\n",
    "            if k == 1: # get VE pairs\n",
    "                pair_VE_V, pair_VE_E = CR.popleft(), ST.popleft()\n",
    "                Forman_VE.append([pair_VE_V, pair_VE_E])\n",
    "            elif k == 2: # get EF pairs\n",
    "                pair_EF = get_EF_pair(ST, crit_cell, Forman_VE, Forman_EF)\n",
    "                if pair_EF: # the EF pair exists\n",
    "                    pair_EF_E = pair_EF[0]\n",
    "                    pair_EF_F = pair_EF[1]\n",
    "                    Forman_EF.append([pair_EF_E, pair_EF_F])\n",
    "                    # remove the paired edges and faces\n",
    "                    CR.remove(pair_EF_E)\n",
    "                    ST.remove(pair_EF_F)\n",
    "                else:\n",
    "                    crit_cell.append(CR.popleft())\n",
    "            elif k == 3: # get FT pairs\n",
    "                pair_FT = get_FT_pair(ST, crit_cell, Forman_EF, Forman_FT)\n",
    "                if pair_FT: # the FT pair exists\n",
    "                    pair_FT_F = pair_FT[0]\n",
    "                    pair_FT_T = pair_FT[1]\n",
    "                    Forman_FT.append([pair_FT_F, pair_FT_T])\n",
    "                    # remove the paired faces and tetra\n",
    "                    CR.remove(pair_FT_F)\n",
    "                    ST.remove(pair_FT_T)\n",
    "                else:\n",
    "                    crit_cell.append(CR.popleft())\n",
    "                    \n",
    "            elif k == 4:\n",
    "                crit_cell.append(CR.popleft())\n",
    "                \n",
    "    return crit_cell, Forman_VE, Forman_EF, Forman_FT\n",
    "                \n",
    "# StructType for Crit_cell        \n",
    "add_Crit_cell_schema = StructType([\n",
    "    StructField('Crit_cell',ArrayType(IntegerType()),True)\n",
    "])\n",
    "\n",
    "# StructType for VE pairs        \n",
    "add_VE_schema = StructType([\n",
    "    StructField(\"VE_pair_V\", IntegerType(),True), \n",
    "    StructField('VE_pair_E',ArrayType(IntegerType()),True)\n",
    "])\n",
    "\n",
    "# StructType for EF pairs        \n",
    "add_multi_EF_schema = StructType([\n",
    "    StructField(\"EF_pair_E\", ArrayType(IntegerType()),True), \n",
    "    StructField('EF_pair_F',ArrayType(IntegerType()),True)\n",
    "])\n",
    "\n",
    "# StructType for FT pairs        \n",
    "add_multi_FT_schema = StructType([\n",
    "    StructField(\"FT_pair_F\", ArrayType(IntegerType()),True), \n",
    "    StructField('FT_pair_T',ArrayType(IntegerType()),True)\n",
    "])\n",
    "\n",
    "# the whole StructType\n",
    "get_Forman_schema = StructType([\n",
    "    StructField(\"Crit_cell\", ArrayType(ArrayType(IntegerType())), True),\n",
    "    StructField(\"VE_pair\", ArrayType(add_VE_schema), True),\n",
    "    StructField(\"EF_pair\", ArrayType(add_multi_EF_schema), True),\n",
    "    StructField(\"FT_pair\", ArrayType(add_multi_FT_schema), True)\n",
    "])\n",
    "\n",
    "# convert a function to an udf and determine the return type\n",
    "# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "get_Forman_udf = udf(get_Forman, get_Forman_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "challenging-medicare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- Forman: struct (nullable = true)\n",
      " |    |-- Crit_cell: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |-- VE_pair: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- VE_pair_V: integer (nullable = true)\n",
      " |    |    |    |-- VE_pair_E: array (nullable = true)\n",
      " |    |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |-- EF_pair: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- EF_pair_E: array (nullable = true)\n",
      " |    |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |    |    |-- EF_pair_F: array (nullable = true)\n",
      " |    |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |-- FT_pair: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- FT_pair_F: array (nullable = true)\n",
      " |    |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |    |    |-- FT_pair_T: array (nullable = true)\n",
      " |    |    |    |    |-- element: integer (containsNull = true)\n",
      "\n",
      "The number of partitions for df_Forman: 400\n"
     ]
    }
   ],
   "source": [
    "df_Forman = df_LS_EFT.withColumn(\"LS_Forman\", get_Forman_udf(df_LS_EFT.Ver, df_LS_EFT.LS_VT, df_LS_EFT.LS_VF, df_LS_EFT.LS_VE))\n",
    "df_Forman = df_Forman.select(col(\"Ver\"), col(\"VT\"), col(\"LS_Forman\").alias(\"Forman\"))\n",
    "df_Forman.printSchema()\n",
    "print(\"The number of partitions for df_Forman:\", df_Forman.rdd.getNumPartitions())\n",
    "# df_Forman.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-oracle",
   "metadata": {},
   "source": [
    "## Methods to build Graph FT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-retreat",
   "metadata": {},
   "source": [
    "#### prepare nodes of Graph FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "addressed-advocate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tetra: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tetra_node = df_tetra_order.select(col(\"r1\"), col(\"r2\"), col(\"r3\"), col(\"r4\"))\n",
    "df_tetra_node = df_tetra_node.withColumn(\"tetra_origin\", F.array(\"r1\", \"r2\", \"r3\", \"r4\"))\n",
    "df_tetra_node = df_tetra_node.withColumn(\"tetra\", sort_array(\"tetra_origin\", False)).drop(\"tetra_origin\") # sort_array(\"tri\", False), False means descending order\n",
    "df_tetra_node = df_tetra_node.select(col(\"tetra\"))\n",
    "df_tetra_node.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "adjusted-novelty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add a special triangle [-1, -1, -1] as node -1; this is useful to deal with boundary elements\n",
    "# e.g., for a boundary edge, it has only one coboundary triangle; to guarantee a consistent encoding, each edge has two coboundary triangels, the other coboundary triangle for a boundary edge is [-1,-1,-1]\n",
    "# columns_special = ['tetra', 'V2_node']\n",
    "value_special = [(-1, -1, -1, -1)]\n",
    "df_special_V2_node = spark.createDataFrame(value_special, ArrayType(IntegerType()))\n",
    "df_special_V2_node.printSchema()\n",
    "# df_special_V2_node.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "renewable-leeds",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_graph_V2_node = df_tetra_node.union(df_special_V2_node)\n",
    "df_graph_V2_node = df_graph_V2_node.withColumn(\"tetra_str\", concat_ws(\",\", col(\"tetra\")))\n",
    "df_graph_V2_node_final = df_graph_V2_node.select(col(\"tetra_str\").alias(\"id\"))\n",
    "df_graph_V2_node_final.printSchema()\n",
    "# df_graph_V2_node_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "musical-mayor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost of df_graph_V2_node_final: 0.7373881340026855\n"
     ]
    }
   ],
   "source": [
    "# time cost to get df_graph_V2_node_final\n",
    "\n",
    "t_df_graph_V2_node_final_0 = time.time()\n",
    "num_df_graph_V2_node_final = df_graph_V2_node_final.count()\n",
    "\n",
    "t_df_graph_V2_node_final_1 = time.time()\n",
    "t_df_graph_V2_node_final = t_df_graph_V2_node_final_1 - t_df_graph_V2_node_final_0\n",
    "print(\"time cost of df_graph_V2_node_final:\", t_df_graph_V2_node_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-hopkins",
   "metadata": {},
   "source": [
    "#### prepare arcs of the Graph FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dirty-border",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- FT_pair: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- FT_pair_F: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |    |-- FT_pair_T: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Forman_FT_pair = df_Forman.select(col(\"Ver\"), col(\"VT\"), col(\"Forman.FT_pair\").alias(\"FT_pair\"))\n",
    "df_Forman_FT_pair.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "marine-warren",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- FT_pair_inTetra: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- FT_pair_co_t: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |    |-- FT_pair_t: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# retrieve co-boundary tetrahedron (FT relation)\n",
    "def get_co_tetra(VT, FT_pair):\n",
    "    if len(FT_pair) > 0:\n",
    "        co_tetra_pair = []\n",
    "        num_FT_pair = len(FT_pair) # obtain the number of FT_pairs\n",
    "        def Co_tetra(f,t,vt):\n",
    "            # f is sorted\n",
    "            for t_temp in vt:\n",
    "                # e0 = sorted([vt[0], vt[1]], reverse=True) # sort the triangle in descending order\n",
    "                f0 = [t_temp[0], t_temp[1], t_temp[2]] # f0, f1, and f2 are in descending order by default\n",
    "                f1 = [t_temp[0], t_temp[1], t_temp[3]]\n",
    "                f2 = [t_temp[0], t_temp[2], t_temp[3]]\n",
    "                f3 = [t_temp[1], t_temp[2], t_temp[3]]\n",
    "                t = [int(t[0]), int(t[1]), int(t[2]), int(t[3])]\n",
    "                t_temp = [int(t_temp[0]), int(t_temp[1]), int(t_temp[2]), int(t_temp[3])]\n",
    "                # if e==e0 or e==e1 or e==e2 and t != t_temp:\n",
    "                if f==f0 or f==f1 or f==f2 or f==f3:\n",
    "                    if t != t_temp:\n",
    "                        return t_temp # t_temp is a coboundary triangle of t in vt\n",
    "                \n",
    "        for i in range(num_FT_pair):\n",
    "            FT_pair_f = FT_pair[i]['FT_pair_F'] # the i-th FT_pair\n",
    "            FT_pair_t = FT_pair[i]['FT_pair_T']\n",
    "            FT_pair_co_t = Co_tetra(FT_pair_f, FT_pair_t, VT)\n",
    "            if FT_pair_co_t is None: # len(FT_pair_co_t) is empty, boundary tetrahedron\n",
    "                FT_pair_co_t = [-1, -1, -1, -1]\n",
    "            co_tetra_pair.append([FT_pair_co_t, FT_pair_t])\n",
    "            \n",
    "        return co_tetra_pair\n",
    "    \n",
    "# the whole StructType\n",
    "get_co_tetra_schema = StructType([\n",
    "    StructField(\"FT_pair_co_t\", ArrayType(IntegerType()), True),\n",
    "    StructField(\"FT_pair_t\", ArrayType(IntegerType()), True)\n",
    "])\n",
    "\n",
    "# convert a function to an udf and determine the return type\n",
    "# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "get_co_tetra_udf = udf(get_co_tetra, ArrayType(get_co_tetra_schema))\n",
    "\n",
    "df_Forman_V2_edge = df_Forman_FT_pair.withColumn(\"FT_pair_inTetra\", get_co_tetra_udf(df_Forman_FT_pair.VT, df_Forman_FT_pair.FT_pair))\n",
    "df_Forman_V2_edge = df_Forman_V2_edge.select(col(\"Ver\"), col(\"FT_pair_inTetra\"))\n",
    "df_Forman_V2_edge.printSchema()\n",
    "# df_Forman_V2_edge.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "painted-bobby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arc_dst: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- arc_src: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explode df_Forman_V2_edge\n",
    "df_Forman_V2_edge_split = df_Forman_V2_edge.select(explode(df_Forman_V2_edge.FT_pair_inTetra).alias(\"FT_pair_src_dst\"))\n",
    "\n",
    "df_Forman_V2_edge_split = df_Forman_V2_edge_split.select(col(\"FT_pair_src_dst.FT_pair_co_t\").alias(\"arc_dst\"), col(\"FT_pair_src_dst.FT_pair_t\").alias(\"arc_src\")).na.drop()\n",
    "df_Forman_V2_edge_split.printSchema()\n",
    "# df_Forman_V2_edge_split.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "administrative-filter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- src: string (nullable = false)\n",
      " |-- dst: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Forman_V2_edge_split = df_Forman_V2_edge_split.withColumn(\"arc_dst_str\", concat_ws(\",\", col(\"arc_dst\")))\n",
    "df_Forman_V2_edge_split = df_Forman_V2_edge_split.withColumn(\"arc_src_str\", concat_ws(\",\", col(\"arc_src\")))\n",
    "df_graph_V2_edge_final = df_Forman_V2_edge_split.select(col(\"arc_src_str\").alias(\"src\"), col(\"arc_dst_str\").alias(\"dst\"))\n",
    "\n",
    "df_graph_V2_edge_final.printSchema()\n",
    "# df_graph_V2_node_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "turkish-wright",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost of df_graph_V2_edge_final: 34.15576934814453\n"
     ]
    }
   ],
   "source": [
    "# time cost to get df_graph_V2_edge_final\n",
    "\n",
    "t_df_graph_V2_edge_final_0 = time.time()\n",
    "num_df_graph_V2_edge_final= df_graph_V2_edge_final.count()\n",
    "\n",
    "t_df_graph_V2_edge_final_1 = time.time()\n",
    "t_df_graph_V2_edge_final = t_df_graph_V2_edge_final_1 - t_df_graph_V2_edge_final_0\n",
    "print(\"time cost of df_graph_V2_edge_final:\", t_df_graph_V2_edge_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-anthony",
   "metadata": {},
   "source": [
    "### construct Graph_FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "valued-cleaners",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of graph_V2: <class 'graphframes.graphframe.GraphFrame'>\n"
     ]
    }
   ],
   "source": [
    "graph_FT = GraphFrame(df_graph_V2_node_final, df_graph_V2_edge_final)\n",
    "\n",
    "print(\"type of graph_V2:\", type(graph_FT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "compliant-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a checkpoint directory to improve performance\n",
    "# Checkpointing regularly helps recover from failures, clean shuffle files, shorten the\n",
    "# lineage of the computation graph, and reduce the complexity of plan optimization.\n",
    "\n",
    "spark.sparkContext.setCheckpointDir('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-knock",
   "metadata": {},
   "source": [
    "### Compute descending 3-manifolds (influence regions of maxima), which are the connected components of Graph_FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "unable-bubble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = false)\n",
      " |-- component: long (nullable = true)\n",
      "\n",
      "time cost of connected components: 124.49729084968567\n"
     ]
    }
   ],
   "source": [
    "t_con_0 = time.time()\n",
    "# result_con consists of two columns, vertex id, component\n",
    "result_con = graph_FT.connectedComponents()\n",
    "result_con.printSchema()\n",
    "\n",
    "t_con_1 = time.time()\n",
    "t_con = t_con_1 - t_con_0\n",
    "print(\"time cost of connected components:\",t_con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-hungary",
   "metadata": {},
   "source": [
    "#### Extract critical triangles and critical tetrahedra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "forced-vietnam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- Forman: struct (nullable = true)\n",
      " |    |-- Crit_cell: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |-- VE_pair: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- VE_pair_V: integer (nullable = true)\n",
      " |    |    |    |-- VE_pair_E: array (nullable = true)\n",
      " |    |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |-- EF_pair: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- EF_pair_E: array (nullable = true)\n",
      " |    |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |    |    |-- EF_pair_F: array (nullable = true)\n",
      " |    |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |-- FT_pair: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- FT_pair_F: array (nullable = true)\n",
      " |    |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |    |    |-- FT_pair_T: array (nullable = true)\n",
      " |    |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_df_Forman = 'Tetra_data' + '/' + tin_filename + '_Forman_VT' + '.parquet'\n",
    "\n",
    "df_Forman = spark.read.parquet(file_df_Forman)\n",
    "df_Forman.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sticky-controversy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions for df_crit: 312\n",
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- Critical: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_crit = df_Forman.select(\"Ver\", \"VT\", col(\"Forman.Crit_cell\").alias(\"Critical\"))\n",
    "print(\"number of partitions for df_crit:\", df_crit.rdd.getNumPartitions())\n",
    "df_crit.printSchema()\n",
    "# df_crit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "becoming-oregon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions for df_crit_VEFT: 312\n",
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- Critical: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- crit_cell: struct (nullable = true)\n",
      " |    |-- crit_ver: integer (nullable = true)\n",
      " |    |-- crit_edge: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |-- crit_tri: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |-- crit_tetra: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split the Critical column to obtain the critical vertices, edges, and triangles\n",
    "def get_crit_VEFT(crit_cell):\n",
    "    if len(crit_cell) < 1: # crit_cell is empty, there are no critical simplices within this lower star\n",
    "        return\n",
    "    else:\n",
    "        # initialize the critical vertices, critical edges, and critical triangles\n",
    "        crit_ver = None\n",
    "        crit_edge = []\n",
    "        crit_tri = []\n",
    "        crit_tetra = []\n",
    "        for icrit_cell_temp in crit_cell:\n",
    "            if len(icrit_cell_temp) == 1: # icrit_cell_temp stores a critical vertex\n",
    "                crit_ver = icrit_cell_temp[0]\n",
    "            if len(icrit_cell_temp) == 2: # icrit_cell_temp stores critical edges\n",
    "                crit_edge.append(icrit_cell_temp)\n",
    "            if len(icrit_cell_temp) == 3: # icrit_cell_temp stores critical triangles\n",
    "                crit_tri.append(icrit_cell_temp)\n",
    "            if len(icrit_cell_temp) == 4: # icrit_cell_temp stores critical tetrahedra\n",
    "                crit_tetra.append(icrit_cell_temp)\n",
    "                \n",
    "        if len(crit_edge) == 0:\n",
    "            crit_edge = None\n",
    "        if len(crit_tri) == 0:\n",
    "            crit_tri = None\n",
    "        if len(crit_tetra) == 0:\n",
    "            crit_tetra = None\n",
    "        return crit_ver, crit_edge, crit_tri, crit_tetra\n",
    "        \n",
    "# the whole StructType\n",
    "get_crit_VEFT_schema = StructType([\n",
    "    StructField(\"crit_ver\", IntegerType(), True),\n",
    "    StructField(\"crit_edge\", ArrayType(ArrayType(IntegerType())), True),\n",
    "    StructField(\"crit_tri\", ArrayType(ArrayType(IntegerType())), True),\n",
    "    StructField(\"crit_tetra\", ArrayType(ArrayType(IntegerType())), True)\n",
    "])\n",
    "\n",
    "# convert a function to an udf and determine the return type\n",
    "# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "get_crit_VEFT_udf = udf(get_crit_VEFT, get_crit_VEFT_schema)\n",
    "\n",
    "df_crit_VEFT = df_crit.withColumn(\"crit_cell\", get_crit_VEFT_udf(df_crit.Critical))\n",
    "print(\"number of partitions for df_crit_VEFT:\", df_crit_VEFT.rdd.getNumPartitions())\n",
    "df_crit_VEFT.printSchema()\n",
    "# df_crit_VEFT.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "designed-switzerland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- Max_tri: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n",
      "number of partitions for df_crit_F: 312\n"
     ]
    }
   ],
   "source": [
    "# to get critical triangles\n",
    "df_crit_F_init = df_crit_VEFT.select(col(\"Ver\"), col(\"VT\"), col(\"crit_cell.crit_tri\").alias(\"Max_tri_init\")).na.drop()\n",
    "\n",
    "# there could be multiple critical triangles within one lower star\n",
    "df_crit_F = df_crit_F_init.select(col(\"Ver\"), col(\"VT\"), explode(\"Max_tri_init\").alias(\"Max_tri\")).na.drop()\n",
    "df_crit_F.printSchema()\n",
    "print(\"number of partitions for df_crit_F:\", df_crit_F.rdd.getNumPartitions())\n",
    "# df_crit_F.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "configured-addition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- Max_tetra: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n",
      "number of partitions for df_crit_T: 312\n"
     ]
    }
   ],
   "source": [
    "# to get critical triangles\n",
    "df_crit_T_init = df_crit_VEFT.select(col(\"Ver\"), col(\"crit_cell.crit_tetra\").alias(\"Max_tetra_init\")).na.drop()\n",
    "\n",
    "# there could be multiple critical triangles within one lower star\n",
    "df_crit_T = df_crit_T_init.select(col(\"Ver\"), explode(\"Max_tetra_init\").alias(\"Max_tetra\")).na.drop()\n",
    "df_crit_T.printSchema()\n",
    "print(\"number of partitions for df_crit_T:\", df_crit_T.rdd.getNumPartitions())\n",
    "# df_crit_T.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "parliamentary-textbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Saddle_CoTetra: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# retrieve co-boundary tetrahedra (FT relation)\n",
    "def get_co_tetra_of_tri(Max_tri, VT):\n",
    "    if len(VT) > 0:\n",
    "        f = Max_tri\n",
    "        tetra_12 = []\n",
    "        num_VT = len(VT) # obtain the number of FT_pairs\n",
    "        for t_temp in VT:\n",
    "            f0 = [t_temp[0], t_temp[1], t_temp[2]] # e0, e1, and e2 are in descending order by default\n",
    "            f1 = [t_temp[0], t_temp[1], t_temp[3]]\n",
    "            f2 = [t_temp[0], t_temp[2], t_temp[3]]\n",
    "            f3 = [t_temp[1], t_temp[2], t_temp[3]]\n",
    "            if f==f0 or f==f1 or f==f2 or f==f3:\n",
    "                tetra_12.append(t_temp)\n",
    "            # if len(tri_12) == 2: # Maybe this triangle is at the boundary, so it has only one corresponding triangle in VT\n",
    "                # break\n",
    "        '''\n",
    "        if len(tetra_12) == 1:\n",
    "            t_special = [-1,-1,-1,-1]\n",
    "            tetra_12.append(t_special)\n",
    "        '''\n",
    "            \n",
    "        return tetra_12\n",
    "    \n",
    "# the whole StructType\n",
    "get_saddle_edge_tri_schema = StructType([\n",
    "    StructField(\"Saddle_CoTetra_t1\", ArrayType(IntegerType()), True),\n",
    "    StructField(\"Saddle_CoTetra_t2\", ArrayType(IntegerType()), True)\n",
    "])\n",
    "\n",
    "# convert a function to an udf and determine the return type\n",
    "# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "get_co_tetra_of_tri_udf = udf(get_co_tetra_of_tri, ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "df_crit_F_tetra = df_crit_F.withColumn(\"Saddle_CoTetra\", get_co_tetra_of_tri_udf(df_crit_F.Max_tri, df_crit_F.VT))\n",
    "df_crit_F_tetra = df_crit_F_tetra.select(col(\"Saddle_CoTetra\"))\n",
    "df_crit_F_tetra.printSchema()\n",
    "# df_crit_E_tri.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "colonial-manor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Saddle_CoTetra_split: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obtain the co-boundary tetrahedra of 2-saddles\n",
    "df_crit_F2tetra = df_crit_F_tetra.select(explode(\"Saddle_CoTetra\").alias(\"Saddle_CoTetra_split\"))\n",
    "# some nodes may belong to multiple saddles, we need to remove them\n",
    "df_crit_F2tetra = df_crit_F2tetra.distinct()\n",
    "df_crit_F2tetra.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-penny",
   "metadata": {},
   "source": [
    "#### 1) left join result_con with 2-saddles tetra to identify the component that each saddles tetra belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "obvious-glass",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions for result_con_saddle_FT: 128\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- component: long (nullable = true)\n",
      " |-- att: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# identify the saddle tetrahedra that each component have, we also need to convert tetraheda data type from array of integers to string at first\n",
    "df_crit_F2tetra = df_crit_F2tetra.withColumn(\"Saddle_CoTetra_split_str\", concat_ws(\",\", col(\"Saddle_CoTetra_split\"))).drop(col(\"Saddle_CoTetra_split\"))\n",
    "\n",
    "result_con_saddle_FT = result_con.join(df_crit_F2tetra,result_con.id==df_crit_F2tetra.Saddle_CoTetra_split_str, \"inner\")\n",
    "result_con_saddle_FT = result_con_saddle_FT.withColumn(\"att\", when(result_con_saddle_FT.Saddle_CoTetra_split_str.isNull() == False, result_con_saddle_FT.Saddle_CoTetra_split_str).otherwise(-1)).drop('Saddle_CoTetra_split_str')\n",
    "\n",
    "print(\"number of partitions for result_con_saddle_FT:\", result_con_saddle_FT.rdd.getNumPartitions())\n",
    "result_con_saddle_FT.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "enormous-sitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- component: long (nullable = true)\n",
      " |-- multi_SdlTetra: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obtain all saddle tetrahedra for each connected component\n",
    "unique_SdlTetra_per_con_FT = result_con_saddle_FT.groupBy('component').agg(collect_list('id').alias('multi_SdlTetra'))\n",
    "unique_SdlTetra_per_con_FT.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-steal",
   "metadata": {},
   "source": [
    "##### 2) left join df_graph_FT_arc with result_con to identify the connected component that each arc belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hairy-korean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- src: string (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      " |-- component: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left join df_graph_V1_arc with result_con to identify which component that each arc belongs to\n",
    "df_G_V2_arc_component = df_graph_FT_arc.join(result_con,df_graph_FT_arc.src==result_con.id, \"left\").drop('id')\n",
    "df_G_V2_arc_component.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-ordinary",
   "metadata": {},
   "source": [
    "##### 3) compute 1-manifolds through Lable Propagation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "encouraging-wrong",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost to find sub_graph_list: 15.856176853179932\n"
     ]
    }
   ],
   "source": [
    "Num_subgraphs = 15\n",
    "t_find_minmax_0 = time.time()\n",
    "\n",
    "# Find the minimum and maximum values of \"component\" in result_con\n",
    "sub_graph_tmp_min = result_con.agg({\"component\": \"min\"}).collect()[0][0]\n",
    "sub_graph_tmp_max = result_con.agg({\"component\": \"max\"}).collect()[0][0]\n",
    "\n",
    "sub_graph_list = []\n",
    "for i in range(Num_subgraphs+1):\n",
    "    sub_graph_tmp = int(i * (sub_graph_tmp_max - sub_graph_tmp_min)/(Num_subgraphs) + sub_graph_tmp_min)\n",
    "    sub_graph_list.append(sub_graph_tmp)    \n",
    "\n",
    "t_find_minmax_1 = time.time()\n",
    "t_find_minmax = t_find_minmax_1 - t_find_minmax_0\n",
    "print(\"time cost to find sub_graph_list:\", t_find_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "retired-fitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty DataFrame with a schema\n",
    "schema_special = StructType([\n",
    "    StructField('id', LongType(),True),\n",
    "    StructField(\"component\", LongType(),True),\n",
    "    StructField(\"att\", LongType(),True),   \n",
    "    StructField(\"label\", LongType(),True)   \n",
    "])\n",
    "\n",
    "df_empty = spark.createDataFrame([], schema_special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "antique-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a checkpoint directory to improve performance\n",
    "# Checkpointing regularly helps recover from failures, clean shuffle files, shorten the\n",
    "# lineage of the computation graph, and reduce the complexity of plan optimization.\n",
    "\n",
    "spark.sparkContext.setCheckpointDir('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eastern-vegetation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost for LPA to subgraph 0: 997.309735\n",
      "Time cost for LPA to subgraph 37795715307: 563.466075\n",
      "Time cost for LPA to subgraph 75591430614: 564.135789\n",
      "Time cost for LPA to subgraph 113387145921: 549.230922\n",
      "Time cost for LPA to subgraph 151182861228: 545.990998\n",
      "Time cost for LPA to subgraph 188978576536: 550.437645\n",
      "Time cost for LPA to subgraph 226774291843: 548.651598\n",
      "Time cost for LPA to subgraph 264570007150: 563.747358\n",
      "Time cost for LPA to subgraph 302365722457: 561.152405\n",
      "Time cost for LPA to subgraph 340161437764: 548.307344\n",
      "Time cost for LPA to subgraph 377957153072: 554.905908\n",
      "Time cost for LPA to subgraph 415752868379: 549.225694\n",
      "Time cost for LPA to subgraph 453548583686: 544.421701\n",
      "Time cost for LPA to subgraph 491344298993: 536.039540\n",
      "Time cost for LPA to subgraph 529140014300: 521.237876\n",
      "*******************************************************\n",
      "Total time cost for LPA: 8698.437216997147\n"
     ]
    }
   ],
   "source": [
    "# construct subgraphs and union the LPA results\n",
    "t_LPA_total_0 = time.time()\n",
    "\n",
    "sub_time = [] # to store time cost for each subgraph\n",
    "for i in range(len(sub_graph_list) - 1):\n",
    "    t_LPA_sub_0 = time.time()\n",
    "    sub_graph_tmp_0 = sub_graph_list[i]\n",
    "    sub_graph_tmp_1 = sub_graph_list[i+1]\n",
    "    \n",
    "    if i == len(sub_graph_list) - 1: # the last subgraphs should contain sub_graph_tmp_max\n",
    "        sub_graph_node = result_con_saddle_FT.filter((result_con_saddle_FT.component >= sub_graph_tmp_0) & (result_con_saddle_FT.component <= sub_graph_tmp_1))\n",
    "        sub_graph_arc = df_G_V2_arc_component.filter((df_G_V2_arc_component.component >= sub_graph_tmp_0) & (df_G_V2_arc_component.component <= sub_graph_tmp_1)).drop('id')\n",
    "    else:\n",
    "        sub_graph_node = result_con_saddle_FT.filter((result_con_saddle_FT.component >= sub_graph_tmp_0) & (result_con_saddle_FT.component < sub_graph_tmp_1))\n",
    "        # sub_graph_node.printSchema() # 3 columns: id, component, att\n",
    "        sub_graph_arc = df_G_V2_arc_component.filter((df_G_V2_arc_component.component >= sub_graph_tmp_0) & (df_G_V2_arc_component.component < sub_graph_tmp_1)).drop('id')\n",
    "        # sub_graph_arc.printSchema() # 3 columns: src, dst, component\n",
    "    \n",
    "    sub_graph = GraphFrame(sub_graph_node, sub_graph_arc)\n",
    "    \n",
    "    sub_graph_LPA = sub_graph.pregel \\\n",
    "         .setMaxIter(10) \\\n",
    "         .withVertexColumn(\"label\", col(\"att\"), \\\n",
    "             Pregel.msg()) \\\n",
    "         .sendMsgToDst(when(Pregel.dst(\"label\")>0, Pregel.dst(\"label\")).otherwise(Pregel.src(\"label\"))) \\\n",
    "         .aggMsgs(F.max(Pregel.msg())) \\\n",
    "         .run()\n",
    "    t_LPA_sub_1 = time.time()\n",
    "    df_empty = df_empty.union(sub_graph_LPA)\n",
    "    print(\"Time cost for LPA to subgraph %ld: %f\" % (sub_graph_tmp_0, t_LPA_sub_1 - t_LPA_sub_0))\n",
    "    sub_time.append(t_LPA_sub_1 - t_LPA_sub_0)\n",
    "    \n",
    "t_LPA_total_1 = time.time()\n",
    "print(\"*******************************************************\")\n",
    "print(\"Total time cost for LPA:\", t_LPA_total_1 - t_LPA_total_0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
